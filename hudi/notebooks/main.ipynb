{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import Row, SparkSession"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "spark_conf = SparkConf()\n",
    "spark_conf.setAll([\n",
    "    (\"spark.app.name\", \"Hudi\"),\n",
    "    (\"spark.ui.showConsoleProgress\", \"true\"),\n",
    "    (\"spark.eventLog.enabled\", \"false\"),\n",
    "    # see https://hudi.apache.org/docs/quick-start-guide#setup\n",
    "    (\"spark.jars\", \",\".join([\n",
    "        \"file://{}\".format(os.path.join(Path().resolve(), \"..\", \"jars\", \"hudi-spark3-bundle_2.12-0.8.0.jar\")),\n",
    "        \"file://{}\".format(os.path.join(Path().resolve(), \"..\", \"jars\", \"spark-avro_2.12-3.1.2.jar\")),\n",
    "    ])),\n",
    "    (\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7fe285045d60>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/08/22 11:08:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "tableName = \"hudi_trips_cow\"\n",
    "basePath = \"file:///workspaces/workspace-lakehouse/hudi/tmp/hudi_trips_cow\"\n",
    "dataGen = spark._jvm.org.apache.hudi.QuickstartUtils.DataGenerator()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "inserts = spark._jvm.org.apache.hudi.QuickstartUtils.convertToStringList(dataGen.generateInserts(10))\n",
    "df = spark.read.json(spark.sparkContext.parallelize(inserts, 2))\n",
    "\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': tableName,\n",
    "    'hoodie.datasource.write.recordkey.field': 'uuid',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'partitionpath',\n",
    "    'hoodie.datasource.write.table.name': tableName,\n",
    "    'hoodie.datasource.write.operation': 'upsert',\n",
    "    'hoodie.datasource.write.precombine.field': 'ts',\n",
    "    'hoodie.upsert.shuffle.parallelism': 2,\n",
    "    'hoodie.insert.shuffle.parallelism': 2\n",
    "}\n",
    "\n",
    "df.write.format(\"hudi\"). \\\n",
    "    options(**hudi_options). \\\n",
    "    mode(\"overwrite\"). \\\n",
    "    save(basePath)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "tripsSnapshotDF = spark. \\\n",
    "  read. \\\n",
    "  format(\"hudi\"). \\\n",
    "  load(basePath + \"/*/*/*/*\")\n",
    "# load(basePath) use \"/partitionKey=partitionValue\" folder structure for Spark auto partition discovery\n",
    "\n",
    "tripsSnapshotDF.createOrReplaceTempView(\"hudi_trips_snapshot\")\n",
    "\n",
    "spark.sql(\"select fare, begin_lon, begin_lat, ts from  hudi_trips_snapshot where fare > 20.0\").show()\n",
    "spark.sql(\"select _hoodie_commit_time, _hoodie_record_key, _hoodie_partition_path, rider, driver, fare from  hudi_trips_snapshot\").show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/08/22 11:11:50 WARN DefaultSource: Loading Base File Only View.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+------------------+-------------------+-------------------+-------------+\n",
      "|              fare|          begin_lon|          begin_lat|           ts|\n",
      "+------------------+-------------------+-------------------+-------------+\n",
      "| 64.27696295884016| 0.4923479652912024| 0.5731835407930634|1629046157276|\n",
      "| 93.56018115236618|0.14285051259466197|0.21624150367601136|1629561586569|\n",
      "| 33.92216483948643| 0.9694586417848392| 0.1856488085068272|1629519091043|\n",
      "| 27.79478688582596| 0.6273212202489661|0.11488393157088261|1629199623019|\n",
      "|  43.4923811219014| 0.8779402295427752| 0.6100070562136587|1629574424275|\n",
      "| 66.62084366450246|0.03844104444445928| 0.0750588760043035|1629467953558|\n",
      "|34.158284716382845|0.46157858450465483| 0.4726905879569653|1629624440054|\n",
      "| 41.06290929046368| 0.8192868687714224|  0.651058505660742|1629235446314|\n",
      "+------------------+-------------------+-------------------+-------------+\n",
      "\n",
      "+-------------------+--------------------+----------------------+---------+----------+------------------+\n",
      "|_hoodie_commit_time|  _hoodie_record_key|_hoodie_partition_path|    rider|    driver|              fare|\n",
      "+-------------------+--------------------+----------------------+---------+----------+------------------+\n",
      "|     20210822111045|02f9c1d9-c682-4c6...|  americas/united_s...|rider-213|driver-213| 64.27696295884016|\n",
      "|     20210822111045|ae182229-91ae-4a2...|  americas/united_s...|rider-213|driver-213| 93.56018115236618|\n",
      "|     20210822111045|c72c5639-789b-45a...|  americas/united_s...|rider-213|driver-213|19.179139106643607|\n",
      "|     20210822111045|a33ef68e-a772-427...|  americas/united_s...|rider-213|driver-213| 33.92216483948643|\n",
      "|     20210822111045|4f46f987-c861-418...|  americas/united_s...|rider-213|driver-213| 27.79478688582596|\n",
      "|     20210822111045|3b095cee-2c52-47c...|  americas/brazil/s...|rider-213|driver-213|  43.4923811219014|\n",
      "|     20210822111045|e4b576e5-fcc5-436...|  americas/brazil/s...|rider-213|driver-213| 66.62084366450246|\n",
      "|     20210822111045|33bcac61-4f1b-404...|  americas/brazil/s...|rider-213|driver-213|34.158284716382845|\n",
      "|     20210822111045|1621525e-5308-475...|    asia/india/chennai|rider-213|driver-213| 41.06290929046368|\n",
      "|     20210822111045|0cd32d2b-fe4b-44e...|    asia/india/chennai|rider-213|driver-213|17.851135255091155|\n",
      "+-------------------+--------------------+----------------------+---------+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "spark.sparkContext._gateway.close()\n",
    "spark.stop()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}